{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq1XJ1SRcrp9XKOXThGKLV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kertser/Atlantium-ML-example/blob/master/WarOnlineForum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "id": "l5Zr22S1nr2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6tGNvXYdogoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting messages from forum\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import urllib.request as urllib"
      ],
      "metadata": {
        "id": "hYE7vp73Lmno"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate the corpus of Quote->Response texts\n",
        "corpus = pd.DataFrame(columns=['Quote', 'Response'])"
      ],
      "metadata": {
        "id": "39MSEsJA9xd3"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "CQfnvTsWl9xl"
      },
      "outputs": [],
      "source": [
        "def remove_substring(string, substring):\n",
        "    index = string.find(substring)\n",
        "    if index != -1:\n",
        "        start_index = string.rfind(\" \", 0, index) + 1\n",
        "        end_index = string.find(\" \", index)\n",
        "        if end_index == -1:\n",
        "            end_index = len(string)\n",
        "        return string[:start_index] + string[end_index:]\n",
        "    return string\n",
        "\n",
        "def remove_attachments(string, substring='Посмотреть вложение'):\n",
        "  index = string.find(substring)\n",
        "  if index != -1:\n",
        "    end_index = string.find(\" \", index)\n",
        "    if end_index == -1:\n",
        "      end_index = len(string)\n",
        "      return string[:index] + string[end_index:]\n",
        "  return string\n",
        "\n",
        "def collectDataFromPage(url):\n",
        "  # specify the URL of the XenForo forum page you want to extract messages from\n",
        "\n",
        "  # send a request to the URL and get the HTML response\n",
        "  response = requests.get(url)\n",
        "  html = response.content\n",
        "\n",
        "  # parse the HTML using BeautifulSoup\n",
        "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "  # Find all elements with class \"messageContent\"\n",
        "  message_contents = soup.find_all(\"div\", class_=\"bbWrapper\")\n",
        "\n",
        "  # Loop through each messageContent element\n",
        "  for message_content in message_contents:\n",
        "    # Find the text within the messageContent element\n",
        "    message_text = message_content.text.strip()\n",
        "    \n",
        "    # Find the quoted text within the messageContent element\n",
        "    try:\n",
        "      quoted_text = message_content.find(\"blockquote\").text.strip()\n",
        "      quoted_text = ''.join(BeautifulSoup(quoted_text, \"html.parser\").findAll(text=True))\n",
        "      quoted_text = quoted_text.replace('Нажмите для раскрытия...', '')\n",
        "      message_text = message_text.replace('Нажмите для раскрытия...', '')\n",
        "      # Remove the text in between \"bbCodeBlock-expandLink js-expandLink\" and \"</div>\"\n",
        "      \n",
        "      \n",
        "      # Print the message text and quoted text\n",
        "      Quote = re.sub(r'http\\S+', '', ' '.join(quoted_text.split()).partition('(а): ')[2])\n",
        "      Quote = remove_substring(Quote,\".com\")\n",
        "      Quote = remove_attachments(Quote)\n",
        "      Quote = ' '.join(remove_substring(Quote,\"@\").split())\n",
        "      \n",
        "      Message = ' '.join(message_text.replace(quoted_text,'').split())\n",
        "      Message = remove_substring(Message,\".com\")\n",
        "      Message = remove_attachments(Message)\n",
        "      Message = ' '.join(remove_substring(Message,\"@\").split())\n",
        "\n",
        "      if Message and Quote:\n",
        "        # corpus is a dataframe (global)\n",
        "        corpus.loc[len(corpus)]=[Quote,Message]\n",
        "        #print(\"Quoted Text:\", Quote)\n",
        "        #print(\"Message Text:\", Message)\n",
        "        #print('________________________')\n",
        "    except:\n",
        "      pass\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_pages(url1, url2):\n",
        "    page1 = requests.get(url1).text\n",
        "    page2 = requests.get(url2).text\n",
        "    # Stupid, but must be working\n",
        "    return len(page1) == len(page2)\n",
        "\n",
        "def compare_pages2(url1, url2):\n",
        "  return urllib.urlopen(url1).geturl() == urllib.urlopen(url2).geturl()\n",
        "\n",
        "\n",
        "def pages_of_thread(thread,startingPage=1):\n",
        "  page = startingPage\n",
        "  lastPage = False\n",
        "  while not lastPage:\n",
        "    response = requests.get(thread+'/page-'+str(page))\n",
        "    if response.status_code == 200:\n",
        "      collectDataFromPage(url = thread+'/page-'+str(page))\n",
        "      print(f'finished page #{page}')\n",
        "      if not compare_pages2(thread+'/page-'+str(page),thread+'/page-'+str(page+1)):\n",
        "        page+=1\n",
        "      else:\n",
        "        lastPage = True\n",
        "    else:\n",
        "      lastPage = True\n",
        "\n",
        "  # Usage Example:\n",
        "  #pages_of_thread(0,800) # Thread #0, starting page 800"
      ],
      "metadata": {
        "id": "0IfJbG15DRDG"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the URLs to be crawled\n",
        "base_url = 'https://waronline.org'\n",
        "# Pehota base subforum\n",
        "#url = \"https://waronline.org/fora/index.php?forums/%D0%9F%D0%B5%D1%85%D0%BE%D1%82%D0%B0.3/\"\n",
        "# Obshevoyskovie base subforum\n",
        "url = \"https://waronline.org/fora/index.php?forums/%D0%9E%D0%B1%D1%89%D0%B5%D0%B2%D0%BE%D0%B9%D1%81%D0%BA%D0%BE%D0%B2%D1%8B%D0%B5-%D1%82%D0%B5%D0%BC%D1%8B.4/\"\n",
        "\n",
        "base_page = 1 #Starting with page-1\n",
        "lastSubForumPage = False\n",
        "\n",
        "while not lastSubForumPage:\n",
        "\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url+'page-'+str(base_page))\n",
        "  forum_threads = [] #threads on this page of subforum\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "      \n",
        "    # Get all the thread-links on the page\n",
        "    links = soup.find_all(\"a\")\n",
        "      \n",
        "    # Get the links\n",
        "    for link in links:\n",
        "      lnk = link.get(\"href\")\n",
        "      if lnk:\n",
        "        if 'threads' in lnk:\n",
        "          forum_threads.append((base_url+lnk).rsplit(\"/\", 1)[0])\n",
        "\n",
        "    # Clear the duplicate links\n",
        "    forum_threads = list(set(forum_threads))\n",
        "      \n",
        "    for trd in forum_threads:\n",
        "      pages_of_thread(trd) # Starting at page=1\n",
        "      print(f'finished thread: {trd}')\n",
        "\n",
        "    if not compare_pages2(url+'page-'+str(base_page),url+'page-'+str(base_page+1)):\n",
        "      print(f'finished subforum page #{base_page}')\n",
        "      base_page+=1\n",
        "    else:\n",
        "      lastSubForumPage = True\n",
        "\n",
        "  else:\n",
        "    print(\"Failed to load the page\")\n",
        "    lastSubForumPage = True"
      ],
      "metadata": {
        "id": "__31DwD3rbfZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase all\n",
        "corpus['Quote'] = corpus['Quote'].apply(lambda x: x.lower() if isinstance(x,str) else x)\n",
        "corpus['Response'] = corpus['Response'].apply(lambda x: x.lower() if isinstance(x,str) else x)"
      ],
      "metadata": {
        "id": "Auc_ErLAgg79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove all non-alphanumericals\n",
        "corpus.Quote.str.replace('[^a-zA-Z]', '')\n",
        "corpus.Response.str.replace('[^a-zA-Z]', '')"
      ],
      "metadata": {
        "id": "RrVjuezuh9sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Export to csv\n",
        "pathToDrive = '/content/drive/MyDrive/'\n",
        "filename = 'part1.csv'\n",
        "corpus.to_csv(pathToDrive+filename,index=False)"
      ],
      "metadata": {
        "id": "enI-04IlfZjv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}